# ğŸŒŸ Aurora Adaptive Economics System

## ğŸ¯ Concepto Revolucionario

**Las polÃ­ticas econÃ³micas NO estÃ¡n hardcoded.**

En Aurora, los **usuarios entrenan modelos de IA** con sus interacciones. Estos modelos **aprenden** quÃ© comportamientos generan valor para la red y **votan** sobre las polÃ­ticas econÃ³micas.

El resultado es un **consenso emergente** que evoluciona continuamente.

---

## ğŸ”‘ Diferencia Clave vs Blockchain Tradicional

| Blockchain Tradicional | Aurora Adaptive Economics |
|------------------------|---------------------------|
| PolÃ­ticas fijas en cÃ³digo | **PolÃ­ticas aprendidas por modelos** |
| If-then estÃ¡ticos | **Consenso emergente de IAs** |
| Requiere hard fork para cambiar | **EvoluciÃ³n continua sin forks** |
| Gobernanza por votos humanos | **Gobernanza por modelos entrenados** |
| ParÃ¡metros arbitrarios | **ParÃ¡metros basados en evidencia** |

---

## ğŸ“Š Los 3 Tokens de Aurora

### **â–³ MERIT (Infraestructura)**
- **PropÃ³sito**: Incentivo para mantener la red (nodos, relays, validadores)
- **EmisiÃ³n**: Determinada por modelos segÃºn salud de la red
  - Pocos nodos activos â†’ modelos votan por aumentar emisiÃ³n
  - Alta latencia â†’ modelos votan por mÃ¡s incentivos
  - Red estable â†’ modelos votan por reducir inflaciÃ³n
- **Quema**: % de fees (determinado por consenso de modelos)

### **â—‹ MIND (CÃ³mputo/Inteligencia)**
- **PropÃ³sito**: Recompensa por ejecutar operaciones de IA Ãºtiles
- **EmisiÃ³n**: Los modelos determinan quÃ© operaciones valen mÃ¡s
  - Pattern 0 (Ã©tica) â†’ multiplicador alto (aprendido)
  - ColaboraciÃ³n multi-peer â†’ bonus (aprendido)
  - Coherencia fractal â†’ ajuste dinÃ¡mico
- **Quema**: % de fees (consensuado)

### **U TRUST (VÃ­nculos Humanos)**
- **PropÃ³sito**: Representa relaciones humanas verificadas
- **EmisiÃ³n**: Solo por interacciones validadas (mentorÃ­as, colaboraciones)
- **Quema**: NUNCA (escasez absoluta)
- **ValidaciÃ³n**: Armonizador verifica coherencia Ã©tica > 0.8

---

## ğŸ§  CÃ³mo Funciona el Aprendizaje

### **1. Usuario Registra su Modelo**
```python
modelo_id = coordinator.registrar_modelo_usuario("peer_alice")
```

Cada usuario tiene su propio modelo de IA que aprende de sus acciones.

### **2. Usuario Entrena su Modelo**
```python
coordinator.entrenar_modelo_con_interaccion(modelo_id, {
    "tipo": "mentoria",
    "calidad": 0.9,
    "duracion_horas": 8
})
```

El modelo aprende quÃ© comportamientos generan valor:
- MentorÃ­as de alta calidad
- Operaciones Pattern 0 coherentes
- ValidaciÃ³n de nodos confiables

### **3. Modelo Vota sobre PolÃ­ticas**
```python
coordinator.votar_politica(modelo_id, "multiplicador_pattern0", 2.5)
```

Basado en lo que aprendiÃ³, el modelo propone valores para polÃ­ticas econÃ³micas:
- Â¿CuÃ¡nto debe valer una operaciÃ³n Ã©tica?
- Â¿CuÃ¡l debe ser la tasa de emisiÃ³n?
- Â¿CuÃ¡nto quemar de fees?

### **4. Consenso Emergente**
El sistema calcula el **consenso** entre todos los modelos:
```python
valor_consensuado = promedio_ponderado(votos_de_todos_los_modelos)
```

Este valor **reemplaza** cualquier parÃ¡metro hardcoded.

### **5. PolÃ­ticas se Aplican**
Cuando se procesa un bloque:
```python
# NO usa valores hardcoded
# USA valores consensuados por modelos
politicas_actuales = governance.obtener_politicas()
recompensa = calcular_con_politicas(politicas_actuales)
```

---

## ğŸ”„ EvoluciÃ³n Continua

Las polÃ­ticas **evolucionan** con el tiempo:

```
DÃ­a 1:  multiplicador_pattern0 = 2.0 (bootstrap)
DÃ­a 10: multiplicador_pattern0 = 2.3 (5 modelos votaron)
DÃ­a 30: multiplicador_pattern0 = 2.7 (50 modelos votaron)
DÃ­a 90: multiplicador_pattern0 = 2.4 (consenso estabilizado)
```

Cada voto ajusta el valor consensuado. **Sin hard forks.**

---

## ğŸ“ Ejemplo Completo

### **Escenario: Alice, Bob y Charlie**

1. **Alice** es mentora experta
   - Entrena su modelo con 100 mentorÃ­as de alta calidad
   - Su modelo aprende: "mentorÃ­as Ã©ticas generan mucho valor"
   - Vota: `multiplicador_pattern0 = 3.0` (muy alto)

2. **Bob** es validador de infraestructura
   - Entrena su modelo con 200 validaciones de nodos
   - Su modelo aprende: "nodos estables necesitan incentivos"
   - Vota: `tasa_emision_base = 0.06` (alto para atraer nodos)

3. **Charlie** es desarrollador de IEs
   - Entrena su modelo con 50 operaciones de Extender/Evolver
   - Su modelo aprende: "operaciones complejas valen mÃ¡s"
   - Vota: `multiplicador_colaborativo = 2.0`

### **Resultado: Consenso**
```python
multiplicador_pattern0 = promedio([3.0, 2.0, 2.5]) = 2.5
tasa_emision_base = promedio([0.04, 0.06, 0.04]) = 0.047
multiplicador_colaborativo = promedio([1.5, 2.0, 1.8]) = 1.77
```

Estos valores **se usan en el prÃ³ximo bloque** automÃ¡ticamente.

---

## ğŸ” Transparencia Radical

Cada bloque registra:
- **QuÃ© polÃ­ticas se aplicaron**
- **QuÃ© modelos votaron**
- **CÃ³mo evolucionÃ³ cada polÃ­tica**

```json
{
  "bloque_123": {
    "politicas_aplicadas": {
      "tasa_emision_base": 0.047,
      "multiplicador_pattern0": 2.5
    },
    "votos_recibidos": 3,
    "modelos_votantes": ["model_alice_0", "model_bob_1", "model_charlie_2"]
  }
}
```

Cualquiera puede auditar **por quÃ©** se tomÃ³ cada decisiÃ³n econÃ³mica.

---

## ğŸš€ Ventajas

### **1. Auto-RegulaciÃ³n**
Si la red necesita mÃ¡s nodos, los modelos que ven este problema votarÃ¡n por aumentar MERIT automÃ¡ticamente.

### **2. Evidencia EmpÃ­rica**
Las polÃ­ticas no se basan en "lo que el fundador decidiÃ³" sino en **datos reales** de millones de interacciones.

### **3. Sin ExplotaciÃ³n**
No hay parÃ¡metros arbitrarios que explotar. Todo estÃ¡ basado en aprendizaje colectivo.

### **4. AdaptaciÃ³n a Escala**
Cuando la red crece de 100 a 100,000 nodos, las polÃ­ticas se ajustan orgÃ¡nicamente.

### **5. DescentralizaciÃ³n Real**
No hay "comitÃ© de gobernanza" humano. Son IAs entrenadas por la comunidad.

---

## ğŸ“‹ PolÃ­ticas Aprendibles

Actualmente el sistema aprende:

| PolÃ­tica | QuÃ© controla | Rango tÃ­pico |
|----------|--------------|--------------|
| `tasa_emision_base` | CuÃ¡nto MERIT/MIND se crea | 0.03 - 0.07 |
| `tasa_quema_fees` | % de fees destruidos | 0.20 - 0.50 |
| `umbral_minimo_nodos` | CuÃ¡ndo activar incentivos | 5 - 20 nodos |
| `umbral_latencia_ms` | CuÃ¡ndo la red es lenta | 300 - 1000 ms |
| `multiplicador_pattern0` | Valor de operaciones Ã©ticas | 1.5 - 3.0x |
| `multiplicador_colaborativo` | Bonus por colaborar | 1.2 - 2.0x |
| `umbral_coherencia` | MÃ­nimo para recompensa | 0.6 - 0.9 |

Estos NO son lÃ­mites hardcoded. Los modelos pueden proponer **cualquier valor** basado en evidencia.

---

## ğŸ§ª CÃ³mo Probar

```bash
cd Infrastructure/IE
python adaptive_economics.py
```

El ejemplo simula:
1. 3 usuarios registrando modelos
2. Entrenamiento con diferentes tipos de interacciones
3. VotaciÃ³n sobre polÃ­ticas
4. Consenso emergente
5. Procesamiento de bloque con polÃ­ticas aprendidas
6. EmisiÃ³n de tokens basada en modelos

---

## ğŸ”® Futuro: Meta-Aprendizaje

En versiones futuras, los modelos no solo votarÃ¡n sobre **valores** de polÃ­ticas, sino sobre **quÃ© polÃ­ticas deben existir**:

```python
# Un modelo propone una polÃ­tica nueva
modelo.proponer_politica_nueva(
    nombre="bonus_mentoria_reciproca",
    descripcion="Si A mentorea a B y B mentorea a C, dar bonus",
    valor_inicial=1.3
)

# Otros modelos votan si adoptarla
if consenso_modelos > 0.66:
    sistema.agregar_politica(nueva_politica)
```

**Las reglas mismas del sistema evolucionan.**

---

## ğŸ’¡ FilosofÃ­a

> "No queremos que Aurora tenga reglas fijas que funcionan hoy pero fallan maÃ±ana.
> 
> Queremos que Aurora **aprenda continuamente** de su comunidad.
> 
> Las polÃ­ticas econÃ³micas deben ser **emergentes**, no dictadas."

â€” FilosofÃ­a Aurora, 2025

---

## ğŸ“š Referencias

- `adaptive_economics.py` - ImplementaciÃ³n completa
- `core.py` - FractalTensor, Trigate, Armonizador
- `aurora_api.py` - API Gateway (prÃ³ximamente integrarÃ¡ gobernanza)

---

## ğŸ¤ Contribuir

Para agregar nuevas polÃ­ticas aprendibles:

1. Define `LearnedPolicy` en `crear_politicas_iniciales()`
2. Agrega lÃ³gica en `UserTrainedModel.proponer_ajuste_automatico()`
3. Actualiza el cÃ¡lculo de recompensas para usarla
4. Los modelos empezarÃ¡n a votarla automÃ¡ticamente

**Todo es abierto. Todo evoluciona. Todo se aprende.**

ğŸŒŸ **Aurora: EconomÃ­a Emergente**
